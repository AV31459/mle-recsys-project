### Ответы на комментарии в ревью_v1 по поводу тестов 

Привет! Мои ответы на твои  
> комментарии по поводу тестов.

даны обычным шрифтом. 

>Прикольно что всё зауниверсалил, чтобы выполнить в одном цикле, но код получился довольно запутанный, если разбирать условия два and и два or. А если захочется добавить еще один тест (в будущем твоему коллеге или тебе), то придется менять этот код вероятно. 
И все таки в настоящем для покрытия кода тестами используют следующие либы:
unittest
pytest
doctest
и другие, но первые две самые популярные.  
Эти библиотеки позволяют оформить тесты в классах/методах, вместо того чтобы писать голый код построчно(так обычно всё же не делают), ну и unittest - это повсеместно используемая фича. Например потому что при нужной настройке CI/CD автотесты можно интегрировать с герритом, для автоматического запуска при комитах, например. То есть кто-то что-то хочет поменять в системе, и автоматом прогонятся твои тесты, чтобы проверить не испортил ли этот человек что-то.

Ок, давай сделаем на стандарном pytest.  
Пояснение, почему было сделано именно так: нестандартный скрипт-раннер был выбран, чтобы можно было кастомизировать вывод(лог) так как нравится.  
Само содержание юнит-тестов (запрос/ответ) лежит в отдельном .json если нужно что-то добавть/убрать сам скрипт трогать не нужно.      

> И еще я бы предложила добавить тестирование глазами, вывести историю пользователя и его персональные рекомендации, И посмотреть соответствуют ли они.

Мы это делали на предыдущем этапе - когда считали (в jupyter) персональные рекомендаций.  
Тестирование "глазами" неформализуемо - однозначного (математического) алгоритма проверки с результатом да/нет (соответствует/не соответствует) не существует.  
Смешивать такую проверку с формальным тестированием, имхо, несколько не корректно.

>И последнее: насколько я поняла(поправь если я не права), ты в некоторых тестах ты проверяешь ожидаемое поведение модели, взяв из файла рекомендации(айдишники) и проверяешь что она выдает те же. Это немножко костыльное решение, допустим представим что в реальной работе, ты модель дообучаешь каждый день, и комитишь. И она уже вполне может выдавать другие рекомендации, и это нормально. То есть более правильно проверять ожидаемое поведение модели, но не в том какие предикты она выдает, на новых пользователей которых нет  в данных ты и проверить не сможешь. Тут именно работоспособность сервиса, ты это всё проверил.

Поправлю: исходно кастомный скрипт поддерживает два режима тестирования (описаны в `README.md`)
- дефолтный: в каждом единичном тесте проверяется http код ответа и количество получаемых рекомендаций (если применимо)
- строгий (`--strict`): будет проверятся точное совпадение получаемых id рекомендаций с эталонными значениями в соответствующем юнит-тесте из файла `test_data.json`. Для использования такого режима сервису нужно давать определенные (фиксированные) оффлайн файлы с данными, а нужен этот режим, например, для проверки корректности алгоритма смешивания онлайн/офлайн рекомендаций и/или проверки отработки по холодным пользователям и т.п.  
